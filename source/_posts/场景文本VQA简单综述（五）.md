---
title: 场景文本VQA简单综述（五）
tags: 场景VQA综述
categories: 论文阅读
cover: url(/img/stvqa.png)
abbrlink: 5a4f99b5
date: 2023-04-12 09:54:29
---

## 相关基本概念补充

### 处理预训练llm微调的三种基本技术

#### 1、fine-tuning

一种自然语言处理技术，用于将预训练的语言模型适应于特定的任务或领域。

经典的fine-tuning方法包括**将预训练模型与少量特定任务数据一起继续训练**。在这个过程中，预训练模型的权重被更新，以更好地适应任务。所需的fine-tuning量取决于预训练语料库和任务特定语料库之间的相似性。如果两者相似，可能只需要少量的fine-tuning。如果两者不相似，则可能需要更多的fine-tuning。

很出名的比如GPT。近几年很流行的大模型使用方法。即将除了输出层以外的所有权重“冻结”（freeze）。然后随机初始化输出层参数，再以迁移学习的方式训练。仅仅更新全连接输出层，其它层的权重不变。

### 2、parameter-efficient fine-tuning

字面意思，就是参数更高效的fine-tuning，简称PEFT，轻量级微调技术，**旨在在尽可能减少所需的参数和计算资源的情况下，实现对预训练语言模型的有效微调**。它是自然语言处理（NLP）中一组用于将预训练语言模型适应特定任务的方法，其所需参数和计算资源比传统的fine-tuning方法更少。

仅通过训练一小组参数来解决传统微调技术需要大量参数的问题，这些参数可能是现有模型参数的子集或新添加的一组参数。这些方法在参数效率、内存效率、训练速度、模型的最终质量和附加推理成本（如果有的话）方面存在差异。

通过直接微调极少量参数就可以在下游任务上得到接近fine-tuning技术的性能。解决上游任务和下游任务输入输出可能存在的结构偏差问题。

比如知识蒸馏、适配器训练、渐进收缩等。

### 3、prompt-tuning技术

重点是调整输入提示（input prompt），预训练模型保持不变，只有输入提示修改以适应下游的任务。通过设计和优化一组提示，可以使预训练模型执行特定任务。

fine-tuning修改模型的权重，而提示调整只修改模型的输入。因此，prompt-tuning调整比精调的计算成本低，需要的资源和训练时间也更少。此外，prompt-tuning比精调更灵活，因为它允许创建特定任务的提示，可以适应各种任务。

#### （1）Prefix tuning（前缀调整）

前缀调整涉及学习特定任务的连续提示，在推理过程中将其添加到输入之前。通过优化这个连续提示，模型可以适应特定任务而不修改底层模型参数，这节省了计算资源并实现了高效的精调。

#### （2）P-tuning 

P-Tuning涉及训练可学习的称为“提示记号”的参数，这些参数与输入序列连接。这些提示记号是特定于任务的，在精调过程中进行优化，使得模型可以在保持原始模型参数不变的情况下在新任务上表现良好。



## Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training

即插即用VQA：通过将大型预训练模型与零训练相结合实现零样本VQA 。

### 摘要

即插即用VQA，PNPVQA需要对预先训练的语言模型（PLM）进行大量的视觉模态调整，而不需要对PLM进行额外的训练。本文**使用自然语言和网络解释作为中间表示**，PNPVQA需要对预先训练的语言模型（PLM）进行大量的视觉模态调整，而不需要对PLM进行额外的训练。PNPVQA需要对预先训练的语言模型（PLM）进行大量的视觉模态调整，而PNPVQA不需要对PLM进行额外的训练。

### 贡献

1、zero-shotVQA模块化框架，无需训练，使PNP-VQA能够随着预训练模型的不断发展而共同发展。

2、我们创建了广泛涵盖问题相关信息的图像标题，从而实现准确的QA。网络接口？

3、我们在多个基准上展示了最先进的零样本VQA性能。没有端到端的训练。

### 多种视觉语言预训练任务

预训练任务包括：

1、image-conditioned language modeling（icim图像条件语言建模）

2、masked language modeling（mlm掩码语言建模）

3、prefix language modeling（plm前缀语言建模）

4、 image-text matching（itm图像文字匹配）

5、image-text contrastive learning（图像文字对比学习）

与上述工作不同，**PNP-VQA直接使用预训练的模型，既没有架构修改，也没有额外的训练**。与我们的工作最为相似的是，PICa（Yang等人，2022）将图像转换为单个字幕，并采用GPT-3（Brown等人，2020）实现零样本VQA。相比之下，PNP-VQA生成多个问题引导的字幕，并在编码后进行字幕融合，以有效利用大量字幕，从而获得可观的性能增益。

### 方法

本文的核心思想是在未受训练的情况下，在预训练的语言模型（*比如llm*）和视觉语言模型之间建立接口。在完全理想的情况下，生成的字幕可以完全覆盖图像中存在的信息。我们通过使用基于显著性图的可解释性技术识别与问题最相关的图像补丁，并仅从这些补丁中生成字幕，来增强相关性。此外，我们通过注入随机性来提高覆盖率，包括在字幕生成期间对相关图像块和文本标记进行随机采样。

主要由以下三个模块组成：

1、图像问题匹配（image-question matching）识别给定问题相关的图像补丁；

2、图像字幕模块，从一组图像补丁中生成不同的字幕组；

3、问答模块，所述问答模块输出给定的问题和所生成的字幕的答案。

![](/img/PNPVQA.png)

#### 图像问题匹配

生成的字幕是描述与问题相关图像区域的字幕，而不是无特定目的的通用标题。这个问题的解决方法是BLIP。采用分支：Image-grounded Text Encoder(ITE)，输出图像v和文本t之间的相似度得分sim（v,t）

采用具有相关热图的通用标题和问题引导标题的示例。



#### 图像字幕模块



太扯了，我去看下代码

接下来的任务：看完这篇论文（仔细）、阅读论文blip-2（简要）、阅读论文fromage、prophet（仔细）

有时间跑一下TAP

未完待续。。。



