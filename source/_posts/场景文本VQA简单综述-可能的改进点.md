---
title: 场景文本VQA简单综述+可能的改进点
tags: 场景VQA综述
abbrlink: 7dabd01
date: 2023-03-31 15:51:15
categories: 论文阅读
cover: url(/img/stvqa.png)
---

## 小综述（ChatGPT）

场景文本VQA（Visual Question Answering）是指在图像中包含文本的情况下，通过问答方式对图像和文本进行分析和理解。场景文本VQA是视觉和自然语言处理（NLP）之间的交叉点，涉及到多个领域的知识和技术，如计算机视觉、自然语言处理、深度学习等。

场景文本VQA的任务是回答与场景相关的问题，这些问题与图像中的文本和图像本身都有关。这些问题可以是关于文本的，也可以是关于图像的，或者是两者的组合。场景文本VQA的**目标是从图像和文本中提取出关键信息，进行推理和推断，最终得出正确的答案。**

场景文本VQA的应用十分广泛，例如在智能交通领域可以用于车牌识别、交通指示牌识别等场景；在图像搜索领域可以用于图像标注、相似图像搜索等场景。此外，场景文本VQA还可以应用于自然语言对话系统、视觉问答等领域。

场景文本VQA的发展面临着一些挑战，如语**言理解、多模态特征融合、多语言场景下的跨语言问题**等。为了解决这些挑战，研究者们提出了各种各样的模型和方法，如基于注意力机制的模型、基于深度学习的模型、迁移学习等方法。随着人工智能技术的不断发展，场景文本VQA的研究和应用将会得到进一步的推进和拓展。

## 目前的进展

对于我现在的研究方向：

阅读vqa论文（未完）+场景文本vqa论文；

完成论文latr阉割版的复现——最后训练的时候爆显； 

总结了一些多模态vqa的论文，有了一些想法（下面）。

之后一周左右的研究方向：

因为现在很多实验有点困难，我打算先看能不能改下模型的网络结构，比如预训练的一些结构：

latr：只有传统的LM模块，空间模块。可以加点图像文本对比（ITC）/匹配（ITM）

## 可能的改进点+问题

对于vqa（包括场景文本vqa）存在普遍问题：

补充：

《Data-Free Adversarial Distillation》论文方法：**该方法通过在原始网络和目标网络之间构建对抗性损失函数，利用目标网络生成虚假的训练数据，并让原始网络去拟合这些虚假数据，从而训练出一个与目标网络具有相似预测性能的轻量级模型**。

一、数据集

数据集难以获取，且噪声较大，我想可不可以使用gan来做辅助，参考2019年一篇icassp论文《Data-Free Adversarial Distillation》，利用知识蒸馏的方法扩充数据集。将自动编码方式与GAN的对抗性训练相结合，训练了能够从高度压缩的特征表示生成图像的网络，以此来扩充数据集。

二、**多模态**

**还是从多模态论文入手**。可以参考METER的方法，对于transformer来说，vit特征比语言模型的特征更为重要，可以考虑在这方面加一点网络结构？只有传统的LM模块，空间模块。



三、跨语言场景

可以将其利用到不同的语言场景？比如中文



## 部分论文+数据集

### ST-VQA——利用图像中的场景文本进行视觉问答



未完待续。。。