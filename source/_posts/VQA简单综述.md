---
title: VQA简单综述
abbrlink: 20961
date: 2023-03-30 10:34:08
tags: VQA综述
categories: 论文阅读
cover: url(/img/ViLT.png)
---

#测试 hexo部署文章	https://blog.csdn.net/K1052176873/article/details/122879462

# 这是一个简单的测试——一些VQA综述

## 1、概述

VL学习：图像字幕、**视觉问答（VQA）**、图像文字匹配

新时代： 通过图像-文本对的预训练来学习视觉和语言的联合表征



## 2、3个阶段：

1、2014-2018年，其间，专门的模型被设计用于不同的任务。无深度学习
2、2019-2021年，在此期间，通过使用有着高质量标签的VL数据集进行预训练，神经网络模型能够学习视觉和语言的联合表征。CNN+RNN **多模态使用**
3、2021年CLIP的出现。此时研究人员寻求在更大的弱标签数据集上预训练VL模型，并通过VL预训练获得性能强大的基于零样本或少样本的视觉模型。**统一架构探索**

### A 图像说明

全局CNN：全局CNN特征有一个明显的弱点，因为解码器不能像人类那样聚焦于图像的重要区域。为解决这个问题，引入了注意机制。
（2018）自底向上/自顶向上（BUTD）

### B VQA

给定一个图像-问题对，视觉问答要求根据图像回答一个问题。大多数研究都将视觉问答视为一个基于预定义答案集的分类问题。
VQA 的总体目标是从图像中提取与问题相关的语义信息，从细微物体的**检测**到抽象场景的**推理**。

## 3、关于VQA的主流模型与方法

-  从问题中提取特征（LSTM, GRU，BERT）
-  从图像中提取特征（VGGNet, ResNet, GoogLeNet, ImageNet）
-  结合这些特征来生成一个答案（目前主要有基于**分类**和**生成**两种方法）

## 4、2021的主流方法

#### CLIP

核心：NLP supervision![[CLIP.jpg]]

#### DALL-E

核心：等下写