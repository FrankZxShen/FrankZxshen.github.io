---
title: 场景文本VQA简单综述（四）
tags: 场景VQA综述
categories: 论文阅读
cover: url(/img/stvqa.png)
abbrlink: 35d15cf2
date: 2023-04-11 17:34:40
---

## 相关基本概念补充

### Learned Query （from ChatGPT）

学习查询(LQ)是一种用于图像检索任务的查询类型，从给定的图像或一组图像中学习查询。学习到的查询可用于**从数据集中检索与输入图像具有类似视觉特征或属性的其他图像**。

LQ通常使用深度神经网络实现。输入图像被馈送到一个网络，该网络**学习图像的视觉特征**的表示。然后使用学习到的表示来生成可用于检索任务的查询。查询可以是固定长度的向量，也可以是单词序列。

LQ已被证明在各种图像检索任务中是有效的，包括基于内容的图像检索、图像字幕和视觉问题回答。



## lmg2llm：From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models

从图像到文本prompts：用冻结的llm进行zero-shot的vqa模型

### 摘要

大型语言模型（LLM）在新的语言任务中表现出了出色的零样本泛化能力。然而，有效利用LLM进行零样本视觉问题回答（VQA）仍然具有挑战性，主要是因为LLM和VQA任务之间的模态断开和任务断开。视觉和语言数据的端到端训练可能会弥合这种脱节，但缺乏灵活性，计算成本高昂。本**文提出Img2Prompt，提供了可以桥接上述模态和任务断开的提示，这样LLM就可以执行零样本VQA任务，而无需进行端到端的训练**。我们进一步采用LLMagnostic模型来提供能够描述图像内容的提示和自我构造的问答对，这可以有效地指导LLM执行零样本VQA任务。

以下好处：1）它可以灵活地与各种LLM一起执行VQA。2） 无需端到端培训，它大大降低了为零样本VQA任务部署LLM的成本。3）  它实现了与依赖端到端训练的方法相当或更好的性能。例如，我们在VQAv2上的表现比Flamingo[3]好5.6%。在具有挑战性的A-OKVQA数据集上，我们的方法甚至比少镜头方法高出20%。

本文的核心方法：**利用视觉语言模型（比如BLIP？？？）和问题生成模型，并将其作为提示提供（Prompt）给llm。**



### 贡献

1、通过仅基于问题的当前图像生成合成问答对，将图像转换为文本提示。Img2Prompt能够弥合语言建模和视觉问答之间的模态脱节和任务脱节。

2、Img2Prompt使LLM能够在**无需昂贵的端到端培训或专门的VQA网络的情况下，离线用于零样本VQA任务。这样做可以降低模型部署成本**，并提供LLM更新的灵活性。

3、效果优于Flamingo。

### 之前的一些工作

文章表明，将llm应用与vqa任务很困难，一种常用的技术是与LLM[3，22，55]联合微调视觉编码器，这使视觉和语言表示空间对齐。但是成本巨高。

**llm for zero/few-shot vqa任务主要分为以下三种**

#### 1、多模式预训练

训练额外的对齐模块和language embedding。llm太大不好微调，只微调视觉编码器。例如Flamingo。缺点：（1）计算效率很低。（2）遗忘。如果LLM与视觉模型联合训练，则对齐步骤可能对LLM的推理能力不利。

![](/img/img2llm-1-1.png)



#### 2、以语言作为中介的VQA

直接采用自然语言作为图像中间表示，不需要昂贵的训练。首先将图像转换为语言描述，将描述提供给llm。于零样本要求，它无法提供上下文内示例，也无法获得上下文内学习的好处。因此，它必须依靠特定于QA的LLM UnifiedQAv2[27]来实现高性能。

![](/img/img2llm-1-2.png)

### 方法

给定一个图像和一个问题Q，从当前图像中生成合成问答对作为无正文示例。

![](/img/img2llm-1-3.png)

#### 1、answer抽取

为了将图像内容纳入上下文学习的范例中，从当前的VQA图像中，我们首先寻找可以作为合成问题答案的单词。

#### 2、问题生成

利用提取的候选答案集{Şaj}Uj=1，我们可以直接使用任何问题生成网络[2，24，28，37，63]来为每个候选答案生成特定的问题。在本文中，我们对基于模板的问题生成方法和神经问题生成方法进行了实验。

#### 3、基于模板的问题生成

使用现成的解析器，我们获得每个答案的词性，并为每个POS类型设计特定的问题模板。例如，对于名词的答案，我们使用“这个图像中的对象是什么？”对于动词的答案，使用“在这个图像中采取了什么行动？”

#### 4、神经问题生成

我们在文本QA数据集上训练了一个神经问题生成模型。具体来说，我们对预先训练的T5大型模型[45]进行微调，以从答案中生成问题。模型的输入包含提示“Answer:[Answer].Context:[Context]”，其中[Answer]表示答案文本，[Context]表示来自文本QA数据集的上下文文本。在推理过程中，我们将[答案]替换为提取的候选答案，将[上下文]替换为从中提取答案的生成标题。该模型在五个文本QA数据集上进行了微调，包括SQuAD2.0[46]、MultiRC[26]、BookQA[43]、CommonsenseQA[53]和Social  IQA

#### 5、与问题相关的标题提示

为了解决这个问题（只关注船没有关注后面的风力涡轮机），我们生成了关于图像中与问题相关的部分的标题，并将其包含在LLM的提示中。 

利用的方法是BLIP的Text Encoder（ITE）将的得到的相似性得分sim（v，q）分配给任意一对图像v和文本问题q。对于ITE，本文利用GradCAM[50]，一种特征属性可解释性技术，来生成粗定位图，突出显示给定问题的匹配图像区域[32]。简言之，GradCam通过ITE相似性函数sim（v，q）相对于交叉注意力得分的梯度来限定来自Transformer网络的交叉注意力得分。

![](/img/img2llm-2.png)

### Prompt设计

通过合成与问题相关的标题和问答对，我们通过连接指令、标题和QA示例来构建LLM的完整提示。说明文本为“请根据上下文对问题的答案进行推理”。标题提示的格式为**“上下文：[所有标题]”“Contexts: [all captions]”**。个别QA示例的格式为**“问题：[问题]答案：[答案]“Question: [question]**
**Answer: [answer]” ”**，并连接在一起。我们将当前问题定位为提示的最后一部分，格式为**“question:[question].Answer:”**。最后，为了得到答案，我们对LLM执行贪婪解码，并像Flamingo中那样删除无意义的令牌。



优先blip2（效果更好）

搭建一个简单的demo（img2llm+blip2）

记得把最好是这篇论文代码的结构给理清楚+blip2

未完待续。。。

