---
title: VQA综述（全）-2019以后
abbrlink: 12980
date: 2023-03-30 16:21:25
tags: VQA综述
categories: 论文阅读
cover: url(/img/ViLT.png)
---

## Transformer解决问题

这块问题是什么呢？（数据集这个可能很难做）主要其实是**方法**，具体一点的就是模型多模态融合的方法。以下是一些可能改进的地方

```
数据集的质量和规模：VQA数据集的质量和规模对模型的性能有很大的影响，数据集的问题质量、答案分布的均衡性、图像质量等因素都会影响模型的表现。目前公开的VQA数据集中，问题的答案往往呈现出极度的不均衡性，例如某些答案的数量比其他答案多出几个数量级，这会导致模型对答案分布不均衡的情况处理不当。

模型的复杂度：VQA任务的复杂度非常高，因为需要同时处理图像和文本信息，这就需要使用很复杂的深度学习模型，例如基于卷积神经网络（CNN）和循环神经网络（RNN）的模型，这些模型需要大量的计算资源和时间来训练和推断。//不考虑用Transformer?

多模态融合：VQA任务需要同时处理图像和文本信息，因此需要进行多模态融合。多模态融合的目的是将图像和文本信息进行融合，以便更好地回答问题。多模态融合的方法有很多，例如将图像特征和文本特征进行拼接、双向注意力机制等，但目前还没有一种通用的多模态融合方法。

常识推理：VQA任务需要对图像和问题进行常识推理，以便回答一些需要深入推理的问题。例如，“这个人正在拿着什么东西？”这个问题需要对场景进行推理，以便判断这个人拿的是什么。

模型的可解释性：VQA任务的结果往往是不确定的，即模型可能回答一个问题但并不能给出确切的答案。因此，为了增加模型的可解释性，需要对模型的推断过程进行解释。
```

总结：回避使用FastRCNN传统目标检测方法识别，使用新工具ViT做多模态特征融合。

以下是一些关于使用Transformer论文的思路

### 传统BERT

一种双向预训练模型，通过在输入文本中加入mask等信息构建句子级和词级的上下文关系，使得文本可以更好地理解文本的含义，BERT使用了一个双向Transformer编码器，并使用了一个基于掩码语言模型（Masked Language Model，MLM）的预训练任务和一个基于下一句预测（Next Sentence Prediction，NSP）的预训练任务。在MLM任务中，BERT随机地将输入文本中的某些单词替换为掩码，并尝试预测这些掩码位置上的单词。

### ViLBERT

![](/img/ViLBER2.png)

最常见的双流模型，BERT 模型（双向 Transformer 结构，利用了 self-attention 来增加上下文的相关性）逐渐成为了语言任务中首选的预训练模型。**该模型在Conceptual Captions数据集上进行预训练，再将其迁移应用到多个视觉-语言任务：视觉问答，视觉常识推理，指示表达(referring expressions)和基于字幕的图像检索**。

ViLBERT修改BERT中query条件下的key-value注意力机制，将其发展成一个多模态共注意transformer模块。在多头注意力中交换的key-value对，该结构使得vision-attended语言特征能够融入入视觉表征(反之亦然)。

整体模型结构由分别 处理图像块 处理文本段（NLP）的两个平行的BERT，个流都是由一系列的transformer blocks和注意力transformer层组成。其attention层是用以两个模态之间特征融合。

![](/img/ViLBERT.png)

#### 两个预训练任务：

1、遮蔽多模态建模

Masked Language Modeling（MLM） 15%（15%进行mask）

Masked Object Classifation（MOC）

使用pretrain的object-detection模型的输出作为ground-truth，以最小化这两个分布的KL散度为目标。

2、预测多模态对齐

Visual-linguistic Matching（VLM）

其目标是预测图像-文本对是否匹配对齐，即本文是否正确的描述了图像。以图像特征序列的起始IMG token和文本序列的起始CLS token的输出作为视觉和语言输入的整体表征。借用vision-and-language模型中另一种常见结构，将IMG token的输出和CLS token的输出进行element-wise product作为最终的总体表征。再利用一个线性层预测图像和文本是否匹配。

![](/img/vi.png)

但是我感觉还是用单流比较好



### VisualBERT

单流

它的方法和我现在用的方法差不多，直接将图片文字放进同一个Transformer做单一的语义学习。

但是还是离开不了Faster-RCNN，用这个东西提取图像边缘特征，加上相应位置编码，片段编码，token/image编码;

其文字部分的输入为原始的 BERT 文字输入（词向量+位置编码+片段编码）加上 Token/Image 编码来表示其是图片或文字

![](/img/visualb.png)

#### 两个预训练任务

与BERT一样，语言掩码+句子-图像预测（VLM）



### Unicoder-VL

单流

Fast-RCNN

差不多

![](/img/Unicoder-vl.png)

还是vit没有出来造成的问题。。。

#### 三个预训练任务

MLM：任务根据上下文推断单词

MOC：对图像一部分mask，任务对图像进行分类（单纯的目标检测）

语言掩码+句子-图像预测（VLM）



。。。

下面这部分是重点，主要在于使用vit框架



### CLIP

可以从大规模未注释图像数据和与之相关的自然语言文本中学习到有用的视觉表示。这种方法利用了图像和自然语言之间的紧密联系，并通过学习对应的视觉和语言表示来提高模型的泛化能力。

一种高效且可转移的图像分类模型。不需要使用大量标注数据进行训练，仅利用自然语言指导和一些简单的数据增强技术就可以达到与使用标注数据进行训练的模型相当的准确率。

![](/img/clip.png)

突出的就是一个泛化性强，一种P-tuning



步骤：

1、首先将一批文字用text Encoder编码（Transformer）,将一批图像用image Encoder编码（VIT或resnet），将所有编码的结果归一化后进行点乘，得到一个相似度矩阵，点乘得到数值越大，表明word和image的向量越相似。监督信号：矩阵对角线为1，其余位置是0？，这个就相当于预训练模型。

2、将预训练好的CLIP迁移到下游任务，先将下游任务的变迁构建为一批带有标签的文本，eg.A photo of {xxx}进行编码。

3、将没有见过的图片进行zero-shot预测（一种机器学习模型，使其可以在没有先前观察过某些类别的情况下对其进行分类。在这种情况下，零样本学习可以帮助模型在测试时正确地分类新类别，而不需要重新训练或收集新的标记数据。零样本学习通常使用属性或语义向量来表示类别，并将它们与输入数据的表示相结合以进行分类），将图片进行一个feature encoder，和上面的一批编码后文本的进行归一化后点积，最后得到的 logits 中数值最大的位置对应的标签即为最终预测结果。

### ViLT

Vision-and-Language Transformer Without Convolution or Region Supervision

目前参数量最小的多模态Transformer方法。ViLT使用预训练的ViT来初始化交互的transformer，这样就可以直接利用交互层来处理视觉特征，不需要额外增加一个视觉encoder（如Faster-RCNN）。

视觉语言模型的三种结构类别：

VE：Vision Embedding **视觉嵌入**

TE：Text Embedding 文本嵌入

MI：Modality Interaction 形态交互

本文的主要贡献在于VE和TE都相当轻量，主要计算集中在模态交互上。

![](/img/ViLT.png)

#### 预训练

老三样

1、（Image Text Matching）ITM

以0.5的概率将文本-图片对中的图片替换其他图片，然后对文本标志位对应输出使用一个线性的ITM head将输出feature映射成一个二值logits，用来判断图像和文本是否匹配。

2、（Mask Languge modeling）MLM

随机mask掉15%的词，使用视觉-文本联合表示来预测。

3、（Whole Word Masking）WWM 最右边的那个

这是一种将词的tokens进行mask的技巧，其目的是使模型充分利用其他模态的信息。

如果并非所有标记都被屏蔽，例如 ["gi", "[MASK]", "##fe"]，则模型可能仅依赖附近的两个语言标记 ["gi", "##fe"] 来预测被屏蔽的“##raf”而不是使用图像中的上下文信息。

整体性能是略低于 region feature 的方法。



先放个大招

### BLIP

Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

2022ICLM 用于统一多模态的理解和生成任务。BLIP可以通过引导字幕，有效地利用有噪声的数据集。



模态混合编码-解码器（MED），这是一个多任务模型，可以在以下三个功能中的一个运行：

1、单模态编码器（Unimodal encoder）：单模态编码器，分别编码图像和文本。文本编码器与BERT相同，其中[CLS]标记被添加在文本输入开头用来总结句子。

2、基于图像的文本编码器（Image-grounded text encoder）：通过在文本编码器的每个transformer块的自注意 (SA) 层和前馈网络 (FFN) 之间插入一个额外的交叉注意 (CA) 层来注入视觉信息。文本中附加了一个特定于任务的[Encode] token，[Encode]的输出嵌入被用作图像-文本对的多模态表示。

3、基于图像的文本解码器（Image-grounded text decoder）：将基于图像的文本编码器中的双向自注意力层替换为因果自注意力层。[Decode] token用于表示序列的开始，而[EOS] token用于表示其结束。

#### 预训练

图像文本对比 Image-Text Contrastive Loss (ITC)

图像文本匹配 Image-Text Matching Loss (ITM)

语言建模 Language Modeling Loss (LM)



![](/img/BLIP.png)

注释-过滤（CapFilt）：解决图像噪声问题

对于一个图片，先使用一个注释器（Captioner）去生成合成注释，然后用一个过滤器（Filter）去除噪声。

![](/img/BLIP2.png)



未完待续。。。