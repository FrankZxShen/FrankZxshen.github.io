---
title: Latr论文思路
abbrlink: 49464
date: 2023-03-30 10:43:00
tags: latr
categories: 论文阅读
cover: url(/img/latr.png)
---

2023-04-04

## LaTr: Layout-Aware Transformer for Scene-Text VQA

### 摘要

本文提出了一种新的用于场景文本视觉问答（STVQA）的多模态构架，称为LayoutAware Transformer（latr）。本文揭示了当文字布局丰富时语言模块的重要性。**提出一种只需要文本和空间线索的单目标预训练方案**。尽管存在领域上的差距，但在扫描文档上应用这种预训练方案与使用自然图像相比具有一定的优势。扫描的文档易于获取，文本密集，具有多种布局，通过将语言和布局信息联系在一起，帮助模型学习各种空间线索。与现有方法相比，我们的方法执行了无词汇解码，并且如图所示，其泛化能力远远超出了训练词汇。同时，Latr还可以提高OCR错误的鲁棒性（OCR错误时STVQA失败的常见原因）。利用VIT，不需要使用FasterRCNN。效果：TextVQA为+7.6%，ST-VQA为+10.8%，OCR-VQA为+4.0%。

![](/img/stvqa.png)

### 研究问题

1、**仅用文本信息回答问题**；

2、**用文本信息和空间布局信息可以回答**；

3、**用文本、空间布局、视觉特征可以回答**。

### 以前存在问题

上面那篇论文**TAP（text-aware pre-training）**存在的问题（*详情见场景文本VQA简单综述（二）*）：

1、**获取大量带有场景文本的自然图片困难**；

2、**获取到的文本比较稀疏**；

3、**在设计预训练目标函数时没有考虑到空间布局信息和语义特征的融合**。

### 贡献

1、**文本和布局信息**在STVQA问题中时很重要的，提出了**Layout-Aware预训练的方法以及网络架构**；

2、采用**扫描的pdf利于结合文本与布局信息**，在其中进行预训练有利于解决STVQA，即使两者之间的问题领域不同；

3、Latr不需要词汇表（用的T5），在训练词汇以外的情况下也表现也表现良好（之前的很差）；一定程度上可以克服OCR错误；

4、效果牛逼（SOTA）

### 模型结构

Latr主要由3个部分组成，首先，是一个只在文本上预先训练的语言模型（左边），将OCR tokens的边界框（bounding box）的空间嵌入与文档上的进一步布局感知（layout-aware）预训练结合使用。

#### 语言模型

本文的Latr基本完全基于T5。T5结构没有改变。在T5的预训练中使用了大量预训练数据，这其中的数据被称为C4。Common Crawl公开可用的网络档案来获得750GB清理过的英语文本数据的子集，他们称之为Colossal Clean Crawled  Corpus（C4）。C4上的预训练是通过去噪任务完成的，这是掩蔽语言建模（MLM）的变体。我们遵循实施并使用HuggingFace中的权重 。

![](/img/latr.png)

#### 二维空间 Embedding

局部信息的价值对于Transformer来说是很重要的。其关键思想是将文本的2-D位置信息预语言标识相关联合耦合。与文档中的单词不同，自然图像中的场景文本可能以任意形状和角度出现（例如，在手表表面）。因此，我们包括文本的高度和宽度，以标识阅读顺序。

如下图所示，给定OCR标记Oi的情况下，相关联的单词边界框可以由（xi0，yi0，xi1，yi1，hi，wi）定义，其中（xi0、yi0）对应于边界框的左上角的位置，（xi1、yi1）表示右下角的位置；（hi，wi）表示相对于阅读顺序的高度和宽度。为了嵌入边界框信息，我们使用了一个查找表，该查找表通常用于对one-hot表示进行连续编码（例如，PyTorch中的nn.Embedding）。

![](/img/latr2d.png)



#### Layout-Aware预训练

总结：只训练具有文本和空间线索的语言模态，以联合建模文本和布局信息之间的交互，进行大量的预训练。

由于T5仅在文本数据上进行训练，我们需要进一步的预训练，以有效地对齐布局信息（2-d空间嵌入）和语义表示。（本文直接在文档中预训练，而不需要img）。它们是各种复杂布局中的富文本环境的来源。我们执行了布局软件去噪预训练任务，其中包括二维空间嵌入，如图所示。这使得能够在预训练阶段使用没有答案注释的弱数据。与正常的去噪任务一样，我们的布局感知去噪任务掩盖了一系列标记，并迫使模型预测掩盖的跨度。与正常的去噪任务不同，我们还允许模型访问屏蔽令牌的粗略位置，这鼓励模型在完成此任务时充分利用布局信息 



2023-03-30

分为两个部分，第一个部分是预训练部分，仅训练具有文本和空格的语言模态（用的ocr数据集），这是一个只在文本上预先训练的语言模型。
接下来，将OCR token令牌边界框的空间嵌入与对文档的进一步布局感知预训练结合使用。

## 预训练

只有传统的LM模块，空间模块。

## 关于训练代码

正式训练过程：其实就是调很多包

### 1、introduction

基于场景文本的VQA任务（多模态体系结构），训练数据集（图像）：TextVQA，并记录相应的权重和偏差。

Latr本质：一种用于做场景文本VQA的布局感知器，用于场景文本可视化问答（STVQA）

优化：提高对OCR错误的鲁棒性，消除对外部物体检测器的需求。

### 2、引入一些包

### 4、制作数据集

将输入预处理为给定的格式，然后将输入提供给模型，得到预处理的输入

### encoding

其中
img has a shape:               torch.Size([3, 384, 500])
boxes has a shape:             torch.Size([512, 6])
tokenized_words has a shape:   torch.Size([512])
question has a shape:          torch.Size([512])
answer has a shape:            torch.Size([512])
id has a shape:                torch.Size([])

### decoding

OCR decoding（1 cup 1 cup 1 cup 250 ml 1 cup 250 ml 1 cup 250 ml 1 cup 1 cup） +
Image decoding（被压扁了。。224*224） +
Question decoding（How big is the measuring cup? ）

### 给dataloader加载Collate function

### 5、define the Datamodule

定义数据集参数，比如训练集、测试集、batch_size啥的


### 6、最终训练阶段

1、定义config文件
2、定义布局VQA训练的各种杂项参数：
输入：boxes、img、question、words ；比如学习率、激活函数、训练损失。。。
3、定义训练主函数
主要参数（max_step：50000，seed：42）
开始训练，保存模型位置”./latr/models“

ok，爆显了，3070真tm的垃圾，又爆显又爆内存，我不玩了

什么时候有3090啊。。。