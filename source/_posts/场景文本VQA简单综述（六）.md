---
title: 场景文本VQA简单综述（六）
date: 2023-04-13 19:31:09
tags: 场景VQA综述
categories: 论文阅读
cover: url(/img/stvqa.png)
---

## 相关基本概念补充

### model-agnostic模型无关

与其说是一个深度学习模型，不如说是一个框架。



## Grounding Language Models to Images for Multimodal Generation

基于语言模型的图像多模态生成

### 核心思想

可以本文的方法引导冻结的大语言模型llm，处理和输出任意交错的多模态数据。**本文通过一个冻结预训练的llm和一个冻结预训练的视觉编码器（相当于没有预训练过程），用多任务目标训练图像字幕和图像文本检索**。对于前者，本文从视觉编码器中提取视觉embedding；对于后者，本文训练语言模型学习一个表示图像的新【RET】标记，并通过对比学习将标题的【RET】嵌入映射为接近其配对图像的视觉嵌入，部分模型都是冻结的，我们只在训练期间更新线性层的权重和【RET】标记嵌入。因此，我们提出的方法是非常节约计算和内存效率。

模型的能力：保留纯文本llm生成文本能力的同时，获得新的多模态对话和推理能力。本文提出的方法是**模型无关（model-agnostic）**的，可以用于未来更强大的llm。

### 贡献

1、提出用于自回归（*GPT*）生成的多模态数据冻结检索（FROMAGe），通过图像字幕和对比学习的冻结的视觉、llm模型。这种方法可以仅从图像描述对中学习到强大的few-shot多模态能力。

2、自回归的llm可以执行文本到图像的检索，对输入文本具有更高的敏感性。与现有模型相比，在长而复杂的自由格式文本上更加精确。

3、预训练的纯文本llm现有的功能，例如上下文学习、输入敏感性以及对话生成，可以用于基于视觉的任务。本文证明：（1）给定交错的图像和文本序列的上下文图像检索（2）视觉对话的zero-shot的强性能（3）提高了图像检索对话上下文的敏感性。

本文为**预训练的纯文本llm在视觉基础任务上的能力**提供了进一步的见解。

### 本文方法

本文的方法集成了语言模型和可视化模型，同时保持它们的参数不变。将图像投射到文本空间（参数转化为线性层），将文本embedding到视觉空间。保持模型冻结的动机是利用从大规模预训练中学到llm的能力。能够更好地泛化到few-shot和zero-shot设置。

本文方法讲究一个多任务：**图像字幕+图像文本检索**

先贴个图：

![](/img/FROMAGE.png)



本文采用的llm是OPT6.7B、视觉encoder是CLIP的ViT-L/14。



明天补充一下。。。