---
title: 场景文本VQA简单综述（三）
tags: 场景VQA综述
categories: 论文阅读
cover: url(/img/stvqa.png)
abbrlink: 82fc4597
date: 2023-04-07 15:36:19
---

## 相关基本概念补充

### zero-shot

用于分类的任务，一次也不学习，在没有任何训练数据的情况下，通过利用类别的语义信息来完成分类任务。

### 预训练&微调

**预训练**：一般的操作都是在一个大型的数据集上（ImageNet）训练一个模型，然后使用该模型作为类似任务的初始化或者特征提取器。**预先训练的一个模型或者指预先训练模型**的过程为预训练

**微调**：将预训练过的模型作用于自己的数据集，并使参数适应自己数据集的过程。接使用之前保存下来的模型的参数来作为这一任务的初始化参数，然后在训练的过程中，依据结果不断进行一些修改。

### VLP（Vision Language Pre-train）

![](/img/vlp.jpg)

VE = Visual Embedding；TE = Text Embedding；MI = Modality Interaction

可以划分为四种

- 重轻轻：VSE、VSE++、SCAN
- 重重轻：CLIP
- 轻重重：ViLBERT、UNTER、Pixel-BERT
- 轻轻重：ViLT

## Latr

### 摘要

本文提出了一种新的用于场景文本视觉问答（STVQA）的多模态构架，称为LayoutAware Transformer（latr）。本文揭示了当文字布局丰富时语言模块的重要性。**提出一种只需要文本和空间线索的单目标预训练方案**。尽管存在领域上的差距，但在扫描文档上应用这种预训练方案与使用自然图像相比具有一定的优势。扫描的文档易于获取，文本密集，具有多种布局，通过将语言和布局信息联系在一起，帮助模型学习各种空间线索。与现有方法相比，我们的方法执行了无词汇解码，并且如图所示，其泛化能力远远超出了训练词汇。同时，Latr还可以提高OCR错误的鲁棒性（OCR错误时STVQA失败的常见原因）。利用VIT，不需要使用FasterRCNN。效果：TextVQA为+7.6%，ST-VQA为+10.8%，OCR-VQA为+4.0%。

![](/img/stvqa.png)

### 研究问题

1、**仅用文本信息回答问题**；

2、**用文本信息和空间布局信息可以回答**；

3、**用文本、空间布局、视觉特征可以回答**。

### 以前存在问题

上面那篇论文**TAP（text-aware pre-training）**存在的问题（*详情见场景文本VQA简单综述（二）*）：

1、**获取大量带有场景文本的自然图片困难**；

2、**获取到的文本比较稀疏**；

3、**在设计预训练目标函数时没有考虑到空间布局信息和语义特征的融合**。

### 贡献

1、**文本和布局信息**在STVQA问题中时很重要的，提出了**Layout-Aware预训练的方法以及网络架构**；

2、采用**扫描的pdf利于结合文本与布局信息**，在其中进行预训练有利于解决STVQA，即使两者之间的问题领域不同；

3、Latr不需要词汇表（用的T5），在训练词汇以外的情况下也表现也表现良好（之前的很差）；一定程度上可以克服OCR错误；

4、效果牛逼（SOTA）

### 模型结构

Latr主要由3个部分组成，首先，是一个只在文本上预先训练的语言模型（左边），将OCR tokens的边界框（bounding box）的空间嵌入与文档上的进一步布局感知（layout-aware）预训练结合使用。

#### 语言模型

本文的Latr基本完全基于T5。T5结构没有改变。在T5的预训练中使用了大量预训练数据，这其中的数据被称为C4。Common Crawl公开可用的网络档案来获得750GB清理过的英语文本数据的子集，他们称之为Colossal Clean Crawled  Corpus（C4）。C4上的预训练是通过去噪任务完成的，这是掩蔽语言建模（MLM）的变体。我们遵循实施并使用HuggingFace中的权重 。

![](/img/latr.png)

#### 二维空间 Embedding

局部信息的价值对于Transformer来说是很重要的。其关键思想是将文本的2-D位置信息预语言标识相关联合耦合。与文档中的单词不同，自然图像中的场景文本可能以任意形状和角度出现（例如，在手表表面）。因此，我们包括文本的高度和宽度，以标识阅读顺序。

如下图所示，给定OCR标记Oi的情况下，相关联的单词边界框可以由（xi0，yi0，xi1，yi1，hi，wi）定义，其中（xi0、yi0）对应于边界框的左上角的位置，（xi1、yi1）表示右下角的位置；（hi，wi）表示相对于阅读顺序的高度和宽度。为了嵌入边界框信息，我们使用了一个查找表，该查找表通常用于对one-hot表示进行连续编码（例如，PyTorch中的nn.Embedding）。

![](/img/latr2d.png)



#### Layout-Aware预训练

总结：只训练具有文本和空间线索的语言模态，以联合建模文本和布局信息之间的交互，进行大量的预训练。

由于T5仅在文本数据上进行训练，我们需要进一步的预训练，以有效地对齐布局信息（2-d空间嵌入）和语义表示。（本文直接在文档中预训练，而不需要img）。它们是各种复杂布局中的富文本环境的来源。我们执行了布局软件去噪预训练任务，其中包括二维空间嵌入，如图所示。这使得能够在预训练阶段使用没有答案注释的弱数据。与正常的去噪任务一样，我们的布局感知去噪任务掩盖了一系列标记，并迫使模型预测掩盖的跨度。与正常的去噪任务不同，我们还允许模型访问屏蔽令牌的粗略位置，这鼓励模型在完成此任务时充分利用布局信息 



## BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

BLIP-2：用冻结的图像编码器和大型语言模型进行语言-图像预训练的自举模型

