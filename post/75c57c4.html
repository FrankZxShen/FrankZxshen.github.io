<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>OCR相关问题 | FrankZxShen's Blog</title><meta name="author" content="FrankZxshen"><meta name="copyright" content="FrankZxshen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="论文用于处理OCR问题Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa超越OCR+VQA：实现端到端阅读和推理，实现稳健准确的文本VQA  不准确的OCR结果会导致错误的累积传播，且文本阅读和基于文本的推理之间的相关性没有得到充分利用。本文旨在是按OCR与VQA任务的">
<meta property="og:type" content="article">
<meta property="og:title" content="OCR相关问题">
<meta property="og:url" content="http://frankzxshen.github.io/post/75c57c4.html">
<meta property="og:site_name" content="FrankZxShen&#39;s Blog">
<meta property="og:description" content="论文用于处理OCR问题Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa超越OCR+VQA：实现端到端阅读和推理，实现稳健准确的文本VQA  不准确的OCR结果会导致错误的累积传播，且文本阅读和基于文本的推理之间的相关性没有得到充分利用。本文旨在是按OCR与VQA任务的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://frankzxshen.github.io/img/fav_maho.png">
<meta property="article:published_time" content="2023-05-25T02:49:19.000Z">
<meta property="article:modified_time" content="2023-06-08T12:15:15.582Z">
<meta property="article:author" content="FrankZxshen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://frankzxshen.github.io/img/fav_maho.png"><link rel="shortcut icon" href="/img/fav_maho.png"><link rel="canonical" href="http://frankzxshen.github.io/post/75c57c4.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'OCR相关问题',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-08 20:15:15'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/fav_maho.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/mahomaho.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="FrankZxShen's Blog"><img class="site-icon" src="/img/fav_maho.png"/><span class="site-name">FrankZxShen's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">OCR相关问题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-25T02:49:19.000Z" title="发表于 2023-05-25 10:49:19">2023-05-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-08T12:15:15.582Z" title="更新于 2023-06-08 20:15:15">2023-06-08</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="论文用于处理OCR问题"><a href="#论文用于处理OCR问题" class="headerlink" title="论文用于处理OCR问题"></a>论文用于处理OCR问题</h3><h2 id="Beyond-OCR-VQA-Towards-end-to-end-reading-and-reasoning-for-robust-and-accurate-textvqa"><a href="#Beyond-OCR-VQA-Towards-end-to-end-reading-and-reasoning-for-robust-and-accurate-textvqa" class="headerlink" title="Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa"></a>Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa</h2><p>超越OCR+VQA：实现端到端阅读和推理，实现稳健准确的文本VQA </p>
<p>不准确的OCR结果会导致错误的累积传播，且文本阅读和基于文本的推理之间的相关性没有得到充分利用。本文旨在是按<strong>OCR与VQA任务的相互强化</strong>，提出了一种视觉增强的文本嵌入模块，用于从文本的视觉信息中预测语义特征。开发了两个方案用于利用VQA中的上下文信息来修改OCR结果。</p>
<p>第一个方案是阅读修改模块，该模块根据上下文自适应地选择答案结果；第二个方案是一种高效的端到端文本阅读和推理网络，下游vqa信号有助于优化文本阅读。</p>
<h3 id="三点贡献："><a href="#三点贡献：" class="headerlink" title="三点贡献："></a><strong>三点贡献：</strong></h3><ol>
<li>本文认为OCR不仅是TextVQA预处理模块，也是使TextVQA与传统VQA不同的关键模块。通<strong>过将OCR集成到TextVQA的流程中，OCR模块与VQA模块相互加强</strong>，有效缓解OCR准确性差对场景推理和问答的限制。</li>
<li>为了抑制OCR错误引起的积累错误传播，提出了<strong>一种视觉增强的文本嵌入模块</strong>（visual enhanced text embedding module），以增强特征表示的鲁棒性以提高推理能力。</li>
<li>为了细化答案预测结果，利用下游推理中的上下文信息来促进文本阅读，在没有 gt OCR注释下，本文设计了一个上下文感知阅读修改模块来修复回答步骤中的OCR错误。当OCR gt可用时，将训练一个有效的端到端网络，通过反馈路径修改阅读结果。</li>
</ol>
<h3 id="本文提出的问题："><a href="#本文提出的问题：" class="headerlink" title="本文提出的问题："></a>本文提出的问题：</h3><p>现在的大部分方法只是将OCR和VQA模块级联，因此它们受到文本阅读性能的显著限制（严重受限于OCR模块的准确度）</p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/BOV++.png"></p>
<p>多模态输入和之前的方法一致，</p>
<ol>
<li><p>question输入：对于问题（question）使用预训练的BERT模型生成问题词嵌入；</p>
</li>
<li><p>视觉对象：Faster R-CNN</p>
</li>
<li><p>OCR对象：一个OCR module用于检测和识别N个OCR tokens。基于识别的结果，提供了多个特征，本文提出了视觉衍生的文本嵌入，用于补充常见的语言嵌入。设计了一个<strong>文本相关的视觉到语义映射网络（TVS）</strong>，直接从文本区域的视觉信息总获得语义嵌入。</p>
</li>
</ol>
<p>最终视觉增强文本嵌入公式化为<br>$$<br>x^{txt}<em>{n}&#x3D;LN(W_4x^v_n+W_5x^{ft}<em>n+W_6x^{p}</em>{n}+W_7x^{fr}</em>{n})+LN(W_8x^b_{n})<br>$$<br>什么意思呢，W都是可学习的投影矩阵，W4的x是表示视觉衍生的文本嵌入，W5的x表示FastText的特征（文本分类），W6表示PHOC（<strong>Pyramidal Histogram of Characters</strong> 字符直方金字塔），W7表示OCR Faster rcnn特征，W8表示OCR边界框特征。</p>
<h4 id="跨模态融合"><a href="#跨模态融合" class="headerlink" title="跨模态融合"></a>跨模态融合</h4><p>Transformer encoder将来自上述三种模态的所有特征都投影到共同的d维嵌入空间，模态间和模态内的交互通过一堆变化层来实现。</p>
<h4 id="Answer-prediction-decoder"><a href="#Answer-prediction-decoder" class="headerlink" title="Answer prediction decoder"></a>Answer prediction decoder</h4><p>M4C</p>
<h4 id="创新网络"><a href="#创新网络" class="headerlink" title="创新网络"></a>创新网络</h4><p>（1）BOV R++</p>
<p>（encoder不变）在解码器开发了一个上下文感知阅读修改模块（CRM） ，在答案解码步骤预测到OCR tokens，通过后处理器CRM来修改其结果；</p>
<p>（2）BOV E++</p>
<p>用端到端的方式级联OCR与VQA模块，下游VQA可以向OCR模块提供反馈，并在训练过程中逐渐验证OCR结构和特征。</p>
<h4 id="视觉增强文本嵌入"><a href="#视觉增强文本嵌入" class="headerlink" title="视觉增强文本嵌入"></a>视觉增强文本嵌入</h4><p>TVS结构</p>
<p>（1）校正模块和编码器（左边）</p>
<p>校正失真文本图像并提取视觉特征</p>
<p>（2）基于GRU decoder预测识别结果</p>
<p>（3）中间语义模块，用于预测单词的全局语义层</p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/TVS.png"></p>
<p>CRM结构</p>
<p>尽管textvqa的推理能力得到了增强，但最终答案预测性能任然高度依赖于OCR模块的准确度。利用（CRM）上下文感知阅读修改模块，改善OCR复制答案。   这是一个后处理器，放在decoder后。</p>
<p>如果解码的答案指向一个OCR标记则为<strong>其准备多个识别的候选词，选择那个候选词是最合适的</strong>。同样这个也是一个Transformer，和一个二分类器结合使用。</p>
<p><strong>CRM的目标是确定当前情况下哪一个候选词是合适的</strong>，</p>
<p>比如，使用Rosetta en作为OCR模块，并提供第一个候选的识别结果。利用预训练的TVS来识别相同的文本区域，rank1和rank2作为提供第二和第三的候选结果。如果一个候选的结果与gt的答案相同，则将其作为正样本。</p>
<p><strong>端到端训练</strong></p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/CRM.png"></p>
<p>该模块提供了一种利用下游上下文线索帮助OCR校正。促进文本阅读和基于文本的推理之间的充分信息交互。将文本阅读的多模态特征纳入TextVQA的训练流程。动态提取OCR结果和特征，打破性能瓶颈。</p>
<p>损失设置</p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/Le2e.png"></p>
<h2 id="From-Token-to-Word-OCR-Token-Evolution-via-Contrastive-Learning-and-Semantic-Matching-for-Text-VQA（TWA）"><a href="#From-Token-to-Word-OCR-Token-Evolution-via-Contrastive-Learning-and-Semantic-Matching-for-Text-VQA（TWA）" class="headerlink" title="From Token to Word: OCR Token Evolution via Contrastive Learning and Semantic Matching for Text-VQA（TWA）"></a>From Token to Word: OCR Token Evolution via Contrastive Learning and Semantic Matching for Text-VQA（TWA）</h2><p>从标记到单词：基于对比学习和文本语义匹配的OCR标记进化VQA </p>
<p>OCR系统中的文本通常包含拼写错误，例如“pepsi”被识别为“peosi”。这些OCR错误是文本VQA系统面临的主要挑战之一。为了解决这一问题，我们提出了一种新的文本VQA方法，通过OCR令牌进化来减轻OCR错误。首先，我们在训练时间内人<strong>为地创建拼写错误的OCR令牌，使系统对OCR错误更加鲁棒。具体来说，我们提出了一种OCR标记词对比（TWC）学习任务，该任务通过OCR标记与词典中单词之间的Levenstein距离来增强OCR标记，从而预训练单词表示。</strong></p>
<p>通过假设拼写错误的OCR令牌中的大多数字符仍然正确，提出了一种多模式转换器，并对其进行了微调，以使用基于字符的单词嵌入来预测答案。具体来说，我们引入了一种具有字符级语义匹配的词汇预测器，即使有拼写错误的OCR标记，该模型也能从词汇中恢复正确的单词。</p>
<h3 id="提出两个问题："><a href="#提出两个问题：" class="headerlink" title="提出两个问题："></a>提出两个问题：</h3><p>1、学习更健壮的OCRtoken表示，当token中存在一些字符错误，应该如何传达正确的语义？两个错误的词比如‘peosi’和‘pepsi’有什么相关性？</p>
<p>2、如何在OCRtoken和单词之间建立正确的联系，使用拼写错误的OCR令牌进一步推断正确的答案？从词汇表中恢复正确的单词。</p>
<h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><p>1、提出了一种新的textvqa框架，用于拼写错误的OCRtoken</p>
<p>2、人工模拟OCR错误，提出了一种OCR标记字 对比学习训练OCR标记的容错表示，帮助推理；</p>
<p>3、提出了一种用于字符级语义匹配的词汇预测器，即使有拼写错误的OCR标记，该模型也能从词汇中恢复正确的单词。 </p>
<p>TWC（OCRtoken-单词对比学习）的任务，该任务通过计算OCRtoken和单词之间的Levenshtein距离增强OCRtoken，从而达到预训练OCRtoken的目的。</p>
<p>给定一个图像和一个问题，使用预训练的OCR模型和obj检测模型对图像进行预处理，获取图像中的文本和对象。使用单词embedding、字符embedding和OCR模型的置信分数来学习OCRtoken的表示。</p>
<p>本文的自监督任务为OCRtoken对比学习任务，在每一步，模型都会从迭代的解码器中预测答案。具体而言，模型从<strong>图像中的OCR标记</strong>、<strong>增强标记</strong>、<strong>训练数据中的常用词词汇表</strong>选择一个单词。由于OCRtoken不总是正确的，本文假设拼写错误的OCRtoken中的大多数字符是正确的，使用基于字符的单词嵌入。</p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/TWA.png"></p>
<p>上面这张图我们可以看出，主要改进有三点：</p>
<p>1、干扰字典中的一些字符（比如减少cool-&gt;col增加and-&gt;andy修改。。。）等方法模拟OCR错误，从而增强OCRtoken。</p>
<p>2、软标签监督OCR标记词对比学习，与其他的预训练任务一起训练。使用迭代解码器进行微调。</p>
<p>3、模型从<strong>图像中的OCR标记</strong>、<strong>增强标记</strong>、<strong>训练数据中的常用词词汇表</strong>选择一个单词，并引入具有字符级语义匹配的指针网络。</p>
<h3 id="多模态方法"><a href="#多模态方法" class="headerlink" title="多模态方法"></a>多模态方法</h3><p>question-&gt;BERT </p>
<p>物体外观特征、边界框坐标-&gt;Faster RCNN</p>
<p>OCR：</p>
<p>视觉特征-&gt;$$x_{n}^{vis}$$</p>
<p>边界框坐标-&gt;$$x_{n}^{bbox}$$</p>
<p>FastText分类特征-&gt;$$x_{n}^{ft}$$</p>
<p>金字塔字符直方图特征-&gt;$$x_{n}^{phoc}$$</p>
<p>本文提出的语义表征-&gt;$$x^{semantic}$$</p>
<p>OCRtoken嵌入：<br>$$<br>x_{n}^{ocr}&#x3D;LN(W_{3}x_{n}^{vis}+W_{4}x_{n}^{phoc}+W_5x_{n}^{ft})+LN(W_6x_{n}^{bbox})+x_n^{semantic}<br>$$<br>最后那个任务如何实现呢？</p>
<p>输入OCRtoken特征与增强的OCRtoken特征，目标是预测每个（OCRtoken，增强token）是否相关。对于每个对，计算token-to-word和word-to-token的相似性。</p>
<h3 id="fineturn"><a href="#fineturn" class="headerlink" title="fineturn"></a>fineturn</h3><p>这里没啥变化，迭代解码的单词来自以下三种：图像中的OCRtoken、增强的token、训练数据的频繁的anwser词汇表。在解码的每一步，首先输入先前预测的单词进行嵌入，并基于transformer预测下一个anwser的单词</p>
<p>好好好，下一篇</p>
<h2 id="Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning（SOHO）"><a href="#Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning（SOHO）" class="headerlink" title="Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning（SOHO）"></a>Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning（SOHO）</h2><p>走出困境：视觉语言表征学习的端到端预训练 </p>
<p>不需要边界框注释？通过促进跨模态的视觉词典（VD）学习全面紧凑的视觉特征。预训练任务掩蔽视觉建模（MVM）</p>
<p>不行，不能用fasterrcnn</p>
<h2 id="Large-Scale-Adversarial-Training-for-Vision-and-Language-Representation-Learning"><a href="#Large-Scale-Adversarial-Training-for-Vision-and-Language-Representation-Learning" class="headerlink" title="Large-Scale Adversarial Training for Vision-and-Language Representation Learning"></a>Large-Scale Adversarial Training for Vision-and-Language Representation Learning</h2><p>视觉和语言表征学习的大规模对抗性训练 </p>
<p>两个训练阶段：</p>
<p>1、任务不可知的（不知道啥是VQA）的对抗性预训练；</p>
<p>2、特定任务的对抗性微调</p>
<p>在每个模态的嵌入空间中加入对抗训练，而不是像素和文本标记上的对抗扰动</p>
<h2 id="Post-OCR-Document-Correction-with-large-Ensembles-of-Character-Sequence-to-Sequence-Models"><a href="#Post-OCR-Document-Correction-with-large-Ensembles-of-Character-Sequence-to-Sequence-Models" class="headerlink" title="Post-OCR Document Correction with large Ensembles of Character Sequence-to-Sequence Models"></a>Post-OCR Document Correction with large Ensembles of Character Sequence-to-Sequence Models</h2><p>利用大型字符序列集合对序列模型进行OCR后文档校正 </p>
<p>这篇是做文档OCR的。</p>
<p>我们提出了一种<strong>基于字符序列模型的任意长度文档校正新方法</strong>。我们方法的新颖之处在于<strong>在短窗口上训练字符序列模型，以检测错误并同时生成候选更正</strong>，而不是像OCR后文本更正系统那样，首先找到错误，然后使用字典或语言模型进行更正。</p>
<p> 1、第一个主要思想是使用序列模型来校正文档的n-gram，而不是将整个文档作为单个序列。以这种方式，可以有效地处理文档，因为并行地校正了n-gram</p>
<p>2、将所有n元校正组合成一个输出，这一过程为该技术增加了鲁棒性，相当于使用大量序列模型的集合，其中每个模型作用于不同的片段。 </p>
<p>3、我们的方法不依赖于预训练的语言模型，这使得它适用于低资源设置而不牺牲性能。 </p>
<p>在合并步骤之后，可以使用字符错误率将最终输出与正确的转录进行比较。 </p>
<p>本文系统的核心是一个标准的seq2seq的模型，可以校正字符序列。用了一个transformer呢。将文档中的一段字符作为数据进行校正。为了训练这个序列模型，有必要将原始文档与其相应的正确转录对齐，这并不总是简单的。由于输出的长度不一定与输入的长度相同（因为可能插入或删除字符），因此需要像贪婪搜索或波束搜索这样的解码方法来根据模型生成最有可能校正的序列。</p>
<p>本文采取的长度维5的n-gram的牛逼方法：</p>
<p><img src="/OCR%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/%E9%95%BF%E5%BA%A6%E4%B8%BA5%E7%9A%84ngram%E6%9B%B4%E6%AD%A3%E6%96%87%E6%A1%A3.png"></p>
<p>n-gram变化的一个重要部分是如何组合部分输出。由于部分校正的偏移量为1，因此可以通过对齐输出并进行投票来组合输出，以获得每个位置最可能的字符。这种投票相当于用n个模型的集合来处理整个输入，每个模型都在偏移量为1的段上操作，其中n是n-gram的阶数。由于在n-gram中间更正了一个字符 。（投票过程省略）</p>
<p>。。。什么玩意。做过实验嘛</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://FrankZxshen.github.io">FrankZxshen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://frankzxshen.github.io/post/75c57c4.html">http://frankzxshen.github.io/post/75c57c4.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://FrankZxshen.github.io" target="_blank">FrankZxShen's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/fav_maho.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/6810bde8.html" title="TAP相关代码详解"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">TAP相关代码详解</div></div></a></div><div class="next-post pull-right"><a href="/post/44a6dc47.html" title="从blip-2得到的启发"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">从blip-2得到的启发</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/fav_maho.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">FrankZxshen</div><div class="author-info__description">今日は頑張ってますか（●´∀｀）ノ♡</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FrankZxShen"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">还在更新维护中...</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E7%94%A8%E4%BA%8E%E5%A4%84%E7%90%86OCR%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">论文用于处理OCR问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beyond-OCR-VQA-Towards-end-to-end-reading-and-reasoning-for-robust-and-accurate-textvqa"><span class="toc-number"></span> <span class="toc-text">Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%82%B9%E8%B4%A1%E7%8C%AE%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">三点贡献：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">本文提出的问题：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%A8%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88"><span class="toc-number">2.1.</span> <span class="toc-text">跨模态融合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Answer-prediction-decoder"><span class="toc-number">2.2.</span> <span class="toc-text">Answer prediction decoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%BD%91%E7%BB%9C"><span class="toc-number">2.3.</span> <span class="toc-text">创新网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E5%A2%9E%E5%BC%BA%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5"><span class="toc-number">2.4.</span> <span class="toc-text">视觉增强文本嵌入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#From-Token-to-Word-OCR-Token-Evolution-via-Contrastive-Learning-and-Semantic-Matching-for-Text-VQA%EF%BC%88TWA%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">From Token to Word: OCR Token Evolution via Contrastive Learning and Semantic Matching for Text-VQA（TWA）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">提出两个问题：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E8%B4%A1%E7%8C%AE"><span class="toc-number">2.</span> <span class="toc-text">本文贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">多模态方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fineturn"><span class="toc-number">4.</span> <span class="toc-text">fineturn</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning%EF%BC%88SOHO%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning（SOHO）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Large-Scale-Adversarial-Training-for-Vision-and-Language-Representation-Learning"><span class="toc-number"></span> <span class="toc-text">Large-Scale Adversarial Training for Vision-and-Language Representation Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Post-OCR-Document-Correction-with-large-Ensembles-of-Character-Sequence-to-Sequence-Models"><span class="toc-number"></span> <span class="toc-text">Post-OCR Document Correction with large Ensembles of Character Sequence-to-Sequence Models</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ad813615.html" title="TWA——Coteaching改进（JoCoR）">TWA——Coteaching改进（JoCoR）</a><time datetime="2023-06-28T13:45:43.000Z" title="发表于 2023-06-28 21:45:43">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/50cbd398.html" title="对抗训练中遇到的问题">对抗训练中遇到的问题</a><time datetime="2023-06-07T08:42:38.000Z" title="发表于 2023-06-07 16:42:38">2023-06-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/6810bde8.html" title="TAP相关代码详解">TAP相关代码详解</a><time datetime="2023-05-31T08:45:11.000Z" title="发表于 2023-05-31 16:45:11">2023-05-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/75c57c4.html" title="OCR相关问题">OCR相关问题</a><time datetime="2023-05-25T02:49:19.000Z" title="发表于 2023-05-25 10:49:19">2023-05-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/44a6dc47.html" title="从blip-2得到的启发">从blip-2得到的启发</a><time datetime="2023-05-22T11:53:41.000Z" title="发表于 2023-05-22 19:53:41">2023-05-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/mahomaho.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By FrankZxshen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div></div></body></html>